%------------------------------------------------------------------------------%
%                                   cholesky                                   %
%------------------------------------------------------------------------------%

\clearpage{}
\section{Cholesky}
\label{sec:cholesky}

The initial task undertaken during the project was the implementation of the
Cholesky decomposition. The work that is described in this section was realized
to demonstrate the capabilities of \gls{hh}, and it was done without any
knowledge of how \gls{fds} operates. For these reasons, we will describe this
work before explaining the principles of the simulator.\\

\gls{fds}'s simulation requires solving equations using the Cholesky
decomposition algorithm. However, the current implementation in the simulation
code is not fast enough. In this section, we will start by explaining the
algorithm. Then we will explain the implementation with \gls{hh} before
presenting the results. For the measurements, our implementation will be
compared to the OpenBLAS's implementation as it is one of the fastest currently
available.

\subsection{The algorithm}
\label{sec:choalgo}

The Cholesky algorithm solves systems of linear equations of the form $Ax = y$,
where $A \in M_{n,n}(\mathrm{R})$ is symmetric and positive-definite
\cite{choleskywiki}. The principle is to decompose the matrix $A$ into a matrix
$L$ such that $A = LL^{T}$, where $L \in M_{n,n}(\mathrm{R})$ is a lower
triangular matrix. The algorithm then solves the two sub-problems: $Lx' = y$ and
$L^{T}x = x'$.

There are several ways to implement this algorithm. For this project, we
implemented the block version as it is the most suited for parallel computing.
Indeed, one of the most important concepts of \gls{hpc} is optimizing CPU cache
usage. To achieve this, we will decompose our matrix into smaller blocks that
can be entirely loaded into the cache, minimizing the need for the CPU to access
the RAM.

The version of the algorithm that we will use is described in
\cite{choleskyblock}. This algorithm proceeds in three steps. First, consider
the matrix $A$ shown in Figure \ref{fig:chodeca}. In this example, $D$
represents a diagonal block, $C$ is a column (composed of multiple blocks) and
$A'$ is a sub-matrix.

% [Cholesky decomposition] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.4]{img/cho-img/cho_block_dec_A.png}
    \caption{Cholesky decomposition: matrix A}
    \label{fig:chodeca}
  \end{center}
\end{figure}
%}}}

The first step of the algorithm is to process the diagonal element $D$. To do
this, we use the sequential version of the algorithm. It will be fast enough
given the size of the block. The second step is to update the elements on the
column beneath the diagonal block, by solving the system $C_{\text{new}} =
C(D^{T})^{-1}$. Once this is done, the algorithm has computed the first column of the
result matrix $L$. The final step consists of updating the rest of the matrix
$A$ by performing $A' = A' - CC^{T}$.

Once we have obtained the matrix $L$, we can proceed to the solving part to find
the solutions of the system. This process can be decomposed in two steps: first,
we solve the equation involving the diagonal block, then we update the rest of
the matrix and the result vector using the partial solution (we remove the
variables from the equation). We repeat these steps for each diagonal block to
solve a sub-problem. The solver must be applied twice to solve the two
sub-problems $Lx' = y$ and $L^{T}x = x'$.

\subsection{Implementation}

The implementation of the algorithm was done in two parts. We began by
implementing the decomposition, and once it was fully functional, the solver
part was added. This approach was chosen because the decomposition is the most
critical part of the algorithm. It was important to compare our version with the
OpenBLAS's one before implementing the solver.

The first implementation of the decomposition involved creating a graph that
followed the steps described in Section \ref{sec:choalgo} similarly to
OpenBLAS. This graph is shown in Figure \ref{fig:chograph}.

% [Cholesky decomposition graph] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.3]{img/cho-img/decompose_graph.png}
    \caption{Cholesky decomposition graph}
    \label{fig:chograph}
  \end{center}
\end{figure}
%}}}

In this diagram, tasks and states are represented by the bold rectangles and the
data by the regular rectangles. There are two kinds of data that flow into this
graph. The first kind is the \texttt{Matrix} that is the input of the graph and
the second kind is the matrix blocks represented by the block's identifier
enclosed in \texttt{'[]'}. The matrix is given to the \texttt{Split Task}, which
generates the blocks. The blocks have identifiers, which define their types.
These identifiers are used to send the right blocks to the appropriate task (or
state). For instance, when the \texttt{Decompose State} sends a
\texttt{Diagonal} block, it is transferred to the \texttt{Compute Diagonal Block
Task}. The execution of this graph can be visualized as follows:

\begin{enumerate}
  \item Split the matrix into blocks (\texttt{Split Matrix Task}).
  \item Compute the diagonal element \textbf{C} (\texttt{Compute Diagonal Block Task}).
  \item Compute the elements on the column \textbf{D} (\texttt{Compute Column Block Task}).
  \item Update the sub-matrix \textbf{A'} (\texttt{Update State / Task}).
  \item Restart from 2 with the blocks of \textbf{A'} (and send the result
    blocks).
\end{enumerate}

This first solution was too slow because the three steps were executed
sequentially (which is also what is done in OpenBLAS). By doing so, we were
loosing a lot of parallelism. For instance, the computation of the diagonal
elements could be done with only one thread and all the other threads had to
wait. To optimize the algorithm, we aimed to keep the threads as busy as
possible, and the solution for this was to rely on early computation by
processing the blocks as soon as they are ready. For instance, we can begin
solving the next diagonal element before all the blocks of the sub-matrix $A'$
are updated. Similarly, we can handle the column the same manner.

To make the early computation possible, the block's data structure was modified.
A \texttt{rank} attribute was added to keep track of the modifications on
the blocks. Each time a block is updated, its rank is incremented. When the rank
of a block is equal to its column number, it is ready to be processed, and it is
processed when its rank is superior to its column number. The states use pending
lists to store indices of blocks that should be processed, and iterate on these
lists to launch the computation of all ready blocks. The graph maintains the
same structure, but the operations in the state have been drastically changed.

In this second version, several verifications are required, using the rank
in the states. Furthermore, both of the states have a vector of block pointers
that are shared between them to make sure that when the \textit{decompose state}
changes a rank, the \textit{update state} can be aware of the modification
directly, without any notification. This improves the performance but adds
complexity in the states.

Eventually, we managed to have good performance with this second version as
OpenBLAS does not make any early computation. We will present our results in the
section \ref{sec:chores}.

For the solver, we use a second graph that is employed twice to solve the two
sub-systems of the matrix. The graph is shown in Figure \ref{fig:solvergraph}.

% [Solver graph] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.3]{img/cho-img/solver_graph.png}
    \caption{Solver graph}
    \label{fig:solvergraph}
  \end{center}
\end{figure}
%}}}

Here, we observe that there are two sub-graphs for the solving part. These two
graphs solve the two sub-problems that we described earlier. The first solver is
directly connected to the decomposition graph, and it receives the decomposed
block. Both solver graphs are connected to the split task, and they receive the
split vector blocks.

In this graph, we use the same matrix blocks as in the decomposition graph,
allowing us to start the solving part directly when the decomposition graph
outputs the first result block. The key principle of \gls{hh} is to allow the
creation of pipelines in which the data flow continuously, so we avoid having
unnecessary barriers\footnote{Points in the program where we must wait for the
completion of task before starting the next one.} in the process. Here, we do
not have to finish the decomposition entirely to start solving the blocks. This
helps to maintain maximal parallelism and ensure the computer's resources are
used efficiently. In other words, if we represented the computation time between
the logical cores in a Gantt chart, our goal would be to optimize the overlap
between tasks and minimize gaps as much as possible (ideally, the cores should
never be in a wait state).

\clearpage{}
\subsection{The results}
\label{sec:chores}

Now that we have explained the algorithm and its implementation, we will analyse
the results of the measurements on the decomposition algorithm using both the
OpenBLAS's implementation and the \gls{hh}'s one. Two sets of measures have been
made on a cluster node with 192 CPU cores. The specifications of the cluster are
listed in Appendix \ref{appendix:sierra}. For the first set, the problem
size was 40,000 and the program has been run with a maximum of 256 threads.
The second set uses a 100,000 matrix and a maximum of 384 threads (utilizing all
the cores). For each problem, the measurements include the computation times of
the two algorithms for different number of threads (from 1 to the maximum) to see
the relative speedup. With \gls{hh}, the number of threads was changed only for
the most critical task (the update task). It was fixed to the optimal numbers
for the other tasks (these numbers were found by doing first tests measures
beforehand).\\

First, let us analyse the measurements made on the 40,000 matrix. Figure
\ref{fig:40000graph} shows the graph generated at the end of the program using
\gls{hh}'s profiling tool. It indicates that the bottleneck of the program is
indeed the update task as we explained before. We can see as well that a single
thread was allocated to the diagonal task and 12 were allocated to the column
task. The diagonal task does not require more than one thread since it processes
at most one block at a time (this is shown by the maximum queue size
\texttt{MQS} on the rectangle before the task). For the column task, it appears
relatively slow (as indicated by the color). We could allocate more threads to
it, but we have to make sure that we keep enough resources for the update task.

% [Hedgehog graph for the 40000 matrix] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.15]{img/cho-img/40000.png}
    \caption{Hedgehog graph for the 40,000 matrix}
    \label{fig:40000graph}
  \end{center}
\end{figure}
%}}}

In Figures \ref{fig:time40000} and \ref{fig:speedups40000} we see the
computation times of the two implementations and the speedup of the \gls{hh}'s
version compared to the OpenBLAS's one for the 40,000 matrix. Here, we observe an
interesting behavior for the OpenBLAS program. As the number threads increases,
the \gls{hh} program continues to scale, while the OpenBLAS's one becomes
unstable around 100 threads. A possible explanation is that OpenBLAS is
optimized for non-hyper-threaded configurations. For these measurements, we
allocated 256 threads on the cluster which means that we could have used 1
thread per core till more than 128 threads were allocated to the program. This
number is quite to the point where the instability begins to appear on the
OpenBLAS's measurements.

\begin{figure}[!htb]
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{img/cho-img/times-40000.png}
    \caption{Computation times for a 40,000 matrix}
    \label{fig:time40000}
  \end{minipage}\hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{img/cho-img/speedup-40000.png}
    \caption{Speedups for a 40,000 matrix (hedgehog / OpenBLAS)}
    \label{fig:speedups40000}
  \end{minipage}
\end{figure}

Figure \ref{fig:relativespeedup40000} shows the relative speedup for each
program ($\frac{time_{one_thread}}{time_{n_thread}}$). As we can see, neither
program achieves a significant speedup. This can be attributed to the fact that
the current problem is too small, limiting resource utilisation. For this
reason, we conducted more measurements with a bigger problem.

% [Relative speedups for a 40000 matrix] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.8]{img/cho-img/relative-speedup-40000.png}
    \caption{Relative speedups for a 40,000 matrix}
    \label{fig:relativespeedup40000}
  \end{center}
\end{figure}
%}}}

The second set of measurements used a $100,000\times100,000$ matrix, and utilized up
to 384 threads. Figures \ref{fig:time100000} and \ref{fig:speedups100000} show
the computation times of the two implementations as well as the speedup of
\gls{hh} against OpenBLAS as observed previously. These two plots show similar
results than the previous ones.

\begin{figure}[!htb]
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{img/cho-img/times-100000.png}
    \caption{Computation times for a 100000 matrix}
    \label{fig:time100000}
  \end{minipage}\hfill
  \begin{minipage}{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{img/cho-img/speedup-100000.png}
    \caption{Speedups for a 100000 matrix (hedgehog / OpenBLAS)}
    \label{fig:speedups100000}
  \end{minipage}
\end{figure}

Figure \ref{fig:relativespeedup100000} shows that we have a way better
relative speedup for the two algorithms. This means that the resources are
better exploited with a bigger problem.

% [Relative speedups for a 100000 matrix] {{{
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.8]{img/cho-img/relative-speedup-100000.png}
    \caption{Relative speedups for a 100000 matrix}
    \label{fig:relativespeedup100000}
  \end{center}
\end{figure}
%}}}

Ultimately, we achieved to have better performance than OpenBLAS by
incorporating more early computation. Additionally, the program remains readable
and easy to understand. This demonstrates the effectiveness of the data-flow
graph approach in parallel computing, as well as the capabilities of the
\gls{hh} library.
